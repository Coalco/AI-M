1...........................................................................................................
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras.layers import Dense

# Данные: Цельсий -> Фаренгейт
celsius = np.array([-40, -10, 0, 8, 15, 22, 38])
fahrenheit = np.array([-40, 14, 32, 46, 59, 72, 100])

# Создание модели
model = keras.Sequential([
    Dense(units=1, input_shape=[1])  # Один полносвязный слой
])

# Компиляция
model.compile(
    loss='mean_squared_error',
    optimizer=keras.optimizers.Adam(0.1)
)

# Обучение
history = model.fit(celsius, fahrenheit, epochs=500, verbose=0)

# График потерь
plt.plot(history.history['loss'])
plt.grid(True)
plt.xlabel("Эпоха")
plt.ylabel("Ошибка")
plt.title("График обучения модели")
plt.show()

# Предсказание для 25°C
input_value = np.array([25]).reshape(-1, 1)
prediction = model.predict(input_value)[0][0]
print(f"25°C = {prediction:.1f}°F")

# Параметры модели (вес и смещение)
weights, bias = model.layers[0].get_weights()
print(f"Веса: {weights[0][0]:.2f}, Смещение: {bias[0]:.2f}")

2...........................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist       
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten
from sklern.metrics import confusion_matrix,classificatin_report

# Загрузка MNIST с явным указанием типа данных 
(x_train, y_train), (x_test, y_test) = mnist.load_data(path='mnist.npz')
x_train = x_train.astype('float32')
x_test= x_test.astype('float32')

# стандартизация входных данных
x_train = x_train / 255
x_test = x_test / 255

y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

#Проверка формы данных
print ("Форма x_train:",x_train.shape)
print ("Форма y_train:",y_train.shape)
print ("Форма x_test:",x_test.shape)
print ("Форма y_test:",y_test.shape)
# отображение первых 25 изображений из обучающей выборки
plt.figure(figsize=(10,5))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(x_train[i], cmap=plt.cm.binary)

plt.show()

model = keras.Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

print(model.summary())      # вывод структуры НС в консоль

model.compile(optimizer='adam',
             loss='categorical_crossentropy',
             metrics=['accuracy'])


model.fit(x_train, y_train_cat, batch_size=32, epochs=5, validation_split=0.2)
loss, accuracy = model.evaluate(x_test, y_test_cat)
print('Test accuracy:', accury)
n = 1
x = np.expand_dims(x_test[n], axis=0)
res = model.predict(x)
print( res )
print( np.argmax(res) )

plt.imshow(x_test[n], cmap=plt.cm.binary)
plt.show()

# Распознавание всей тестовой выборки
pred = model.predict(x_test)
pred = np.argmax(pred, axis=1)

print(pred.shape)

print(pred[:20])
print(y_test[:20])

# Выделение неверных вариантов
mask = pred == y_test
print(mask[:10])
if np.any(~mask):
   x_false = x_test[~mask]
   y_false = x_test[~mask]
   print(x_false.shape)

# Вывод первых 25 неверных результатов
plt.figure(figsize=(10,5))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(x_false[i], cmap=plt.cm.binary)

   plt.show()
else:
print("Нет неверных классификаций!")

# Дополнительная оценка:Отчет классификации 
print ( classification_report(y_test,pred))
# Дополнительная оценка: матрица неточностей
print(confusion_matrix(y_test, pred)

3...........................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping # Ранняя
остановка
# Загрузка MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Нормализация данных
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)
# Определение размеров выборок
limit = 5000
x_train_data = x_train[:limit]
y_train_data = y_train_cat[:limit]
x_valid = x_train[limit:limit*2]
y_valid = y_train_cat[limit:limit*2]
# Архитектура модели
model = keras.Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(300, activation='relu'),
    Dropout(0.3),  # Уменьшаем Dropout до 0.3
    Dense(10, activation='softmax')
])
# Компиляция модели
model.compile(optimizer='adam',
             loss='categorical_crossentropy',
             metrics=['accuracy'])
# Ранняя остановка
early_stopping = EarlyStopping(monitor='val_loss', patience=10,
restore_best_weights=True)
# Обучение модели
his = model.fit(x_train_data, y_train_data, epochs=50,
batch_size=32, validation_data=(x_valid, y_valid),
callbacks=[early_stopping])
# График обучения
plt.plot(his.history['loss'], label='Обучающая выборка')
plt.plot(his.history['val_loss'], label='Валидационная выборка')
plt.xlabel('Эпоха')
plt.ylabel('Функция потерь')
plt.legend()
plt.title('Кривая обучения')
plt.show()
# Оценка на тестовой выборке
loss, accuracy = model.evaluate(x_test, y_test_cat, verbose=0)
print('Точность на тестовой выборке: %f' % (accuracy))

Batch Normalization (батч-нормализация)
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten,
BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
# Загрузка данных
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Нормализация данных
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
# Преобразование меток в категориальный вид
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)
# Разделение на обучающую и валидационную выборки
limit = 5000
x_train_data = x_train[:limit]
y_train_data = y_train_cat[:limit]
x_valid = x_train[limit:limit*2]
y_valid = y_train_cat[limit:limit*2]
# Создание модели
model = keras.Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(300),  # Без activation, чтобы BN стоял правильно
    BatchNormalization(),
    keras.layers.Activation('relu'),
    Dense(10, activation='softmax')  # Финальный softmax слой
])
# Компиляция модели
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10,
restore_best_weights=True)
# Обучение модели
his = model.fit(x_train_data, y_train_data, epochs=100,
batch_size=32,
                validation_data=(x_valid, y_valid),
callbacks=[early_stopping])
# Графики обучения
plt.plot(his.history['loss'], label='Обучающая выборка')
plt.plot(his.history['val_loss'], label='Валидационная выборка')
plt.xlabel('Эпоха')
plt.ylabel('Функция потерь')
plt.legend()
plt.show()

4...........................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Dropout,
Conv2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
# Загрузка данных
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Нормализация данных
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
# Преобразование меток в категориальный вид
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)
# Расширение размерности для соответствия входному формату Conv2D
x_train = np.expand_dims(x_train, axis=3)
x_test = np.expand_dims(x_test, axis=3)
print( x_train.shape )
# Создание модели
model = keras.Sequential([
    Conv2D(32, (3,3), padding='same', activation='relu',
input_shape=(28, 28, 1)),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5), # Dropout для регуляризации
    Dense(10, activation='softmax')
])
# Компиляция модели
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10,
restore_best_weights=True)
# Обучение модели
his = model.fit(x_train, y_train_cat, batch_size=32, epochs=20,
validation_split=0.2, callbacks=[early_stopping])
# Оценка на тестовой выборке
loss, accuracy = model.evaluate(x_test, y_test_cat, verbose=0)
print('Точность на тестовой выборке: %f' % (accuracy))
# Графики обучения
plt.plot(his.history['accuracy'], label='Обучающая выборка
(Точность)')
plt.plot(his.history['val_accuracy'], label='Валидационная выборка
(Точность)')
plt.xlabel('Эпоха')
plt.ylabel('Точность')
plt.legend()
plt.show()
plt.plot(his.history['loss'], label='Обучающая выборка (Потеря)')
plt.plot(his.history['val_loss'], label='Валидационная выборка
(Потеря)')
plt.xlabel('Эпоха')
plt.ylabel('Функция потерь')
plt.legend()
plt.show()

5...........................................................................................................
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from google.colab import files
from io import BytesIO
from PIL import Image

# Загрузка предварительно обученной модели VGG16
model = keras.applications.VGG16()

# Загрузка файла
try:
    uploaded = files.upload()
    img = Image.open(BytesIO(uploaded['2.jpg']))

    # Проверка и изменение размера изображения, если необходимо
    img = img.resize((224, 224))  # Явно изменяем размер до 224x224
    plt.imshow( img )
    plt.show()

    # Преобразование изображения в NumPy-массив и предварительная обработка
    img = np.array(img)
    x = keras.applications.vgg16.preprocess_input(img)
    print(x.shape)
    x = np.expand_dims(x, axis=0)

    # Прогнозирование класса изображения
    res = model.predict(x)
    predicted_class_index = np.argmax(res)

    # Декодирование результата
    decoded_predictions = keras.applications.vgg16.decode_predictions(res, top=5)[0] # top=5 возвращает 5 лучших результатов

    print("Предсказания:")
    for i, (imagenet_id, label, probability) in enumerate(decoded_predictions):
        print(f"{i+1}: {label} ({probability:.4f})")

    # Вывод наиболее вероятного класса
    print(f"\nНаиболее вероятный класс: {decoded_predictions[0][1]}")

except FileNotFoundError:
    print("Файл '2.jpg' не найден. Пожалуйста, загрузите изображение.")
except Exception as e:
    print(f"Произошла ошибка: {e}")

6...........................................................................................................
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
from io import BytesIO
from PIL import Image
import tensorflow as tf
from tensorflow import keras
upl = files.upload()
img = Image.open(BytesIO(upl['nach.jpg']))
img_style = Image.open(BytesIO(upl['kon.jpg']))
plt.subplot(1, 2, 1)
plt.imshow( img )
plt.subplot(1, 2, 2)
plt.imshow( img_style )
plt.show()
x_img =
keras.applications.vgg19.preprocess_input( np.expand_dims(img,
axis=0) )
x_style =
keras.applications.vgg19.preprocess_input(np.expand_dims(img_style,
axis=0))
def deprocess_img(processed_img):
  x = processed_img.copy()
  if len(x.shape) == 4:
    x = np.squeeze(x, 0)
  assert len(x.shape) == 3, ("Input to deprocess image must be an
image of "
                             "dimension [1, height, width, channel]
or [height, width, channel]")
  if len(x.shape) != 3:
    raise ValueError("Invalid input to deprocessing image")
 
  # perform the inverse of the preprocessing step
  x[:, :, 0] += 103.939
  x[:, :, 1] += 116.779
  x[:, :, 2] += 123.68
  x = x[:, :, ::-1]
  x = np.clip(x, 0, 255).astype('uint8')
  return x
vgg = keras.applications.vgg19.VGG19(include_top=False,
weights='imagenet')
vgg.trainable = false
# Content layer where will pull our feature maps
content_layers = ['block5_conv2']
# Style layer we are interested in
style_layers = ['block1_conv1',
                'block2_conv1',
                'block3_conv1',
                'block4_conv1',
                'block5_conv1'
               ]
num_content_layers = len(content_layers)
num_style_layers = len(style_layers)
style_outputs = [vgg.get_layer(name).output for name in
style_layers]
content_outputs = [vgg.get_layer(name).output for name in
content_layers]
model_outputs = style_outputs + content_outputs
print(vgg.input)
for m in model_outputs:
  print(m)
model = keras.models.Model(vgg.input, model_outputs)
for layer in model.layers:
    layer.trainable = false
print(model.summary())      # вывод структуры НС в консоль
def get_feature_representations(model):
  # batch compute content and style features
  style_outputs = model(x_style)
  content_outputs = model(x_img)
 
  # Get the style and content feature representations from our
model  
  style_features = [style_layer[0] for style_layer in
style_outputs[:num_style_layers]]
  content_features = [content_layer[0] for content_layer in
content_outputs[num_style_layers:]]
  return style_features, content_features
def get_content_loss(base_content, target):
  return tf.reduce_mean(tf.square(base_content - target))
def gram_matrix(input_tensor):
  # We make the image channels first
  channels = int(input_tensor.shape[-1])
  a = tf.reshape(input_tensor, [-1, channels])
  n = tf.shape(a)[0]
  gram = tf.matmul(a, a, transpose_a=True)
  return gram / tf.cast(n, tf.float32)
def get_style_loss(base_style, gram_target):
  gram_style = gram_matrix(base_style)
 
  return tf.reduce_mean(tf.square(gram_style - gram_target))
def compute_loss(model, loss_weights, init_image,
gram_style_features, content_features):
  style_weight, content_weight = loss_weights
 
  model_outputs = model(init_image)
 
  style_output_features = model_outputs[:num_style_layers]
  content_output_features = model_outputs[num_style_layers:]
 
  style_score = 0
  content_score = 0
  # Accumulate style losses from all layers
  # Here, we equally weight each contribution of each loss layer
  weight_per_style_layer = 1 / float(num_style_layers)
  for target_style, comb_style in zip(gram_style_features,
style_output_features):
    style_score += weight_per_style_layer *
get_style_loss(comb_style[0], target_style)
   
  # Accumulate content losses from all layers
  weight_per_content_layer = 1 / float(num_content_layers)
  for target_content, comb_content in zip(content_features,
content_output_features):
    content_score += weight_per_content_layer*
get_content_loss(comb_content[0], target_content)
 
  style_score *= style_weight
  content_score *= content_weight
  # Get total loss
  loss = style_score + content_score
  return loss, style_score, content_score
num_iterations=100
content_weight=1e3
style_weight=1e-2
style_features, content_features =
get_feature_representations(model)
gram_style_features = [gram_matrix(style_feature) for style_feature
in style_features]
init_image = np.copy(x_img)
init_image = tf.Variable(init_image, dtype=tf.float32)
opt = tf.compat.v1.train.AdamOptimizer(learning_rate=2, beta1=0.99,
epsilon=1e-1)
iter_count = 1
best_loss, best_img = float('inf'), None
loss_weights = (style_weight, content_weight)
cfg = {
      'model': model,
      'loss_weights': loss_weights,
      'init_image': init_image,
      'gram_style_features': gram_style_features,
      'content_features': content_features
}
norm_means = np.array([103.939, 116.779, 123.68])
min_vals = -norm_means
max_vals = 255 - norm_means
imgs = []
for i in range(num_iterations):
    with tf.GradientTape() as tape:
       all_loss = compute_loss(**cfg)
   
    loss, style_score, content_score = all_loss
    grads = tape.gradient(loss, init_image)
    opt.apply_gradients([(grads, init_image)])
    clipped = tf.clip_by_value(init_image, min_vals, max_vals)
    init_image.assign(clipped)
   
    if loss < best_loss:
      # Update best loss and best image from total loss.
      best_loss = loss
      best_img = deprocess_img(init_image.numpy())
      # Use the .numpy() method to get the concrete numpy array
      plot_img = deprocess_img(init_image.numpy())
      imgs.append(plot_img)
      print('Iteration: {}'.format(i))
plt.imshow(best_img)
print(best_loss)
image = Image.fromarray(best_img.astype('uint8'), 'RGB')
image.save("result.jpg")
files.download("result.jpg")

7...........................................................................................................
from keras.layers import Conv2D, UpSampling2D, InputLayer
from keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img # <---- ИСПРАВЛЕНИЕ ЗДЕСЬ
from skimage.color import rgb2lab, lab2rgb
from skimage.io import imsave
import numpy as np
from google.colab import files
from io import BytesIO
from PIL import Image
import matplotlib.pyplot as plt

# --- Функции ---
def processed_image(img):
    """Преобразует изображение в формат, подходящий для модели."""
    image = img.resize((256, 256), Image.BILINEAR)
    image = np.array(image, dtype=float)
    size = image.shape
    lab = rgb2lab(1.0/255*image)  # Нормировка и преобразование в Lab
    X, Y = lab[:,:,0], lab[:,:,1:]

    Y /= 128  # Нормируем выходные значение в диапазон от -1 до 1
    X = X.reshape(1, size[0], size[1], 1)
    Y = Y.reshape(1, size[0], size[1], 2)
    return X, Y, size

def postprocess_image(X, ab, size):
    """Преобразует выходные данные модели обратно в RGB."""
    cur = np.zeros((size[0], size[1], 3))
    cur[:,:,0] = np.clip(X[0][:,:,0], 0, 100)
    cur[:,:,1:] = ab
    return lab2rgb(cur)

# --- Модель ---
model = Sequential()
model.add(InputLayer(input_shape=(256, 256, 1))) # фиксированный размер
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))
model.add(UpSampling2D((2, 2)))

model.compile(optimizer='adam', loss='mse')
model.summary() # Полезно посмотреть структуру

# --- Обучение на одном изображении---
upl = files.upload()
names = list(upl.keys())
img = Image.open(BytesIO(upl[names[0]]))
X, Y, size = processed_image(img)
model.fit(x=X, y=Y, batch_size=1, epochs=50) # ОЧЕНЬ МАЛО ДАННЫХ!

# --- Предсказание ---
upl = files.upload()
names = list(upl.keys())
img = Image.open(BytesIO(upl[names[0]]))
X, Y, size = processed_image(img) # Предварительная обработка как и при обучении

output = model.predict(X)

output *= 128 # Обратная нормализация
min_vals, max_vals = -128, 127
ab = np.clip(output[0], min_vals, max_vals)

# --- Отображение ---
colored_image = postprocess_image(X, ab, size)

plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.imshow(img)
plt.title("Исходное изображение")
plt.subplot(1, 2, 2)
plt.imshow(colored_image)  # lab2rgb уже встроен в postprocess_image
plt.title("Раскрашенное изображение")
plt.show()

8...........................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
import re
from tensorflow.keras.layers import Dense, SimpleRNN, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from google.colab import files # Добавляем импорт
# Загрузка файла
try:
    uploaded = files.upload()
    file_name = next(iter(uploaded)) # Получаем имя загруженного файла
    # Загрузка и предобработка текста
    with open(file_name, 'r', encoding='utf-8') as f: # Используем имя
загруженного файла
        text = f.read()
        text = text.replace('\ufeff', '')  # убираем первый невидимый
символ
        text = re.sub(r'[^А-я ]', '', text)  # заменяем все символы
кроме кириллицы на пустые символы
    # Параметры
    num_characters = 34  # 33 буквы + пробел
    inp_chars = 6
    # Токенизация
    tokenizer = Tokenizer(num_words=num_characters, char_level=True,
oov_token="<UNK>")  # Добавляем oov_token
    tokenizer.fit_on_texts([text])  # формируем токены
    print(tokenizer.word_index)
    # Формирование данных для обучения
    data = tokenizer.texts_to_matrix(text)  # Преобразуем текст в OHE
    n = data.shape[0] - inp_chars  # Количество примеров
    X = np.array([data[i:i + inp_chars, :] for i in range(n)])
    Y = data[inp_chars:]  # предсказание следующего символа
    print(data.shape)
    # Создание модели
    model = Sequential()
    model.add(Input((inp_chars, num_characters)))
    model.add(SimpleRNN(128, activation='tanh')) # RNN слой
    model.add(Dense(num_characters, activation='softmax'))
    model.summary()
    # Компиляция и обучение модели
    model.compile(loss='categorical_crossentropy',
metrics=['accuracy'], optimizer='adam')
    history = model.fit(X, Y, batch_size=32, epochs=100)
    # Функция для генерации текста
    def buildPhrase(inp_str, str_len=50):
        """Генерирует текст на основе входной строки."""
        result = inp_str
        for i in range(str_len):
            # Берем последние inp_chars символов
            tail = result[-inp_chars:]
            # Токенизируем последние символы
            tokens = tokenizer.texts_to_sequences([tail])  #
to_sequences вместо texts_to_matrix
            #Если есть неизвестные токены, то заменяем их на токен для
неизвестного символа
            tokens = [[tokenizer.word_index.get(c,
tokenizer.word_index["<UNK>"]) for c in seq] for seq in tokens]
            # Преобразуем в OHE
            x = to_categorical(tokens[0], num_classes=num_characters)
# to_categorical из keras.utils
            # Дополняем нулями, если длина меньше inp_chars
(маловероятно, но для надежности)
            if x.shape[0] < inp_chars:
                padding = np.zeros((inp_chars - x.shape[0],
num_characters))
                x = np.concatenate([padding, x])
            # Изменяем форму для подачи в модель
            x = np.expand_dims(x, axis=0)
            # Предсказываем следующий символ
            pred = model.predict(x)
            next_char_index = np.argmax(pred)
            next_char = tokenizer.index_word.get(next_char_index,
"<UNK>")  # Получаем символ по индексу, обрабатываем UNK
            result += next_char # Добавляем к результату
        return result
    # Пример использования
    res = buildPhrase("утренн")
    print(res)
except FileNotFoundError as e:
    print(f"Ошибка: Файл не найден. Пожалуйста, загрузите файл
'train_data_true'.")
except Exception as e:
    print(f"Произошла ошибка: {e}")

9...........................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import re

from tensorflow.keras.layers import Dense, SimpleRNN, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from google.colab import files # Добавляем импорт

# Загрузка файла
try:
    uploaded = files.upload()
    file_name = next(iter(uploaded)) # Получаем имя загруженного файла

    # Загрузка и предобработка текста
    with open(file_name, 'r', encoding='utf-8') as f: # Используем имя загруженного файла
        text = f.read()
        text = text.replace('\ufeff', '')  # убираем первый невидимый символ
        text = re.sub(r'[^А-я ]', '', text)  # заменяем все символы кроме кириллицы на пустые символы

    # Параметры
    num_characters = 34  # 33 буквы + пробел
    inp_chars = 6

    # Токенизация
    tokenizer = Tokenizer(num_words=num_characters, char_level=True, oov_token="<UNK>")  # Добавляем oov_token
    tokenizer.fit_on_texts([text])  # формируем токены
    print(tokenizer.word_index)

    # Формирование данных для обучения
    data = tokenizer.texts_to_matrix(text)  # Преобразуем текст в OHE
    n = data.shape[0] - inp_chars  # Количество примеров

    X = np.array([data[i:i + inp_chars, :] for i in range(n)])
    Y = data[inp_chars:]  # предсказание следующего символа

    print(data.shape)

    # Создание модели
    model = Sequential()
    model.add(Input((inp_chars, num_characters)))
    model.add(SimpleRNN(128, activation='tanh')) # RNN слой
    model.add(Dense(num_characters, activation='softmax'))
    model.summary()

    # Компиляция и обучение модели
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')

    history = model.fit(X, Y, batch_size=32, epochs=100)

    # Функция для генерации текста
    def buildPhrase(inp_str, str_len=50):
        """Генерирует текст на основе входной строки."""
        result = inp_str
        for i in range(str_len):
            # Берем последние inp_chars символов
            tail = result[-inp_chars:]

            # Токенизируем последние символы
            tokens = tokenizer.texts_to_sequences([tail])  # to_sequences вместо texts_to_matrix

            #Если есть неизвестные токены, то заменяем их на токен для неизвестного символа
            tokens = [[tokenizer.word_index.get(c, tokenizer.word_index["<UNK>"]) for c in seq] for seq in tokens]

            # Преобразуем в OHE
            x = to_categorical(tokens[0], num_classes=num_characters) # to_categorical из keras.utils

            # Дополняем нулями, если длина меньше inp_chars (маловероятно, но для надежности)
            if x.shape[0] < inp_chars:
                padding = np.zeros((inp_chars - x.shape[0], num_characters))
                x = np.concatenate([padding, x])

            # Изменяем форму для подачи в модель
            x = np.expand_dims(x, axis=0)

            # Предсказываем следующий символ
            pred = model.predict(x)
            next_char_index = np.argmax(pred)
            next_char = tokenizer.index_word.get(next_char_index, "<UNK>")  # Получаем символ по индексу, обрабатываем UNK

            result += next_char # Добавляем к результату

        return result

    # Пример использования
    res = buildPhrase("утренн")
    print(res)

except FileNotFoundError as e:
    print(f"Ошибка: Файл не найден. Пожалуйста, загрузите файл 'train_data_true'.")
except Exception as e:
    print(f"Произошла ошибка: {e}")

10..........................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy
from google.colab import files
try:
    # Загрузка файлов
    uploaded = files.upload()
    texts_true_file = next(iter(uploaded))
    texts_false_file = next(iter(uploaded)) #Загружаем второй файл
    # Чтение данных
    with open(texts_true_file, 'r', encoding='utf-8') as f:
        texts_true = f.readlines()
        texts_true[0] = texts_true[0].replace('\ufeff', '')
    with open(texts_false_file, 'r', encoding='utf-8') as f:
        texts_false = f.readlines()
        texts_false[0] = texts_false[0].replace('\ufeff', '')
    texts = texts_true + texts_false
    count_true = len(texts_true)
    count_false = len(texts_false)
    total_lines = count_true + count_false
    print(count_true, count_false, total_lines)
    # Параметры
    maxWordsCount = 1000
    max_text_len = 20  # Увеличили длину
    # Токенизация
    tokenizer = Tokenizer(num_words=maxWordsCount,
                          filters='!–"—#$%&amp;()*+,-./:;<=>?
@[\\]^_`{|}~\t\n\r«»',
                          lower=True, split=' ', char_level=False,
                          oov_token="<UNK>")  # Добавили oov_token
    tokenizer.fit_on_texts(texts)
    dist = list(tokenizer.word_counts.items())
    print(dist[:10])
    print(texts[0][:100])
    # Формирование последовательностей и padding
    data = tokenizer.texts_to_sequences(texts)
    data_pad = pad_sequences(data, maxlen=max_text_len)
    print(data_pad)
    print(list(tokenizer.word_index.items()))
    # Формирование X и Y
    X = data_pad
    Y = np.array([[1, 0]] * count_true + [[0, 1]] * count_false)
    print(X.shape, Y.shape)
    # Перемешивание данных
    indeces = np.random.choice(X.shape[0], size=X.shape[0],
replace=False)
    X = X[indeces]
    Y = Y[indeces]
    # Создание модели
    model = Sequential()
    model.add(Embedding(maxWordsCount, 128,
input_length=max_text_len))
    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.5))  # Добавили Dropout
    model.add(LSTM(64))
    model.add(Dropout(0.5))  # Добавили Dropout
    model.add(Dense(2, activation='softmax'))
    model.summary()
    # Компиляция и обучение
    model.compile(loss='categorical_crossentropy',
metrics=['accuracy'], optimizer=Adam(0.0001))
    history = model.fit(X, Y, batch_size=32, epochs=50)
    # Обратный словарь
    reverse_word_map = dict(map(reversed,
tokenizer.word_index.items()))
    def sequence_to_text(list_of_indices):
        words = [reverse_word_map.get(letter) or '<UNK>' for letter
in list_of_indices] # обработка UNK
        return ' '.join(words) # соединяем в строку
    # Пример использования
    t = "Я люблю позитивное настроение".lower()
    data = tokenizer.texts_to_sequences([t])
    data_pad = pad_sequences(data, maxlen=max_text_len)
    print(sequence_to_text(data_pad[0])) # Правильно используем
data_pad
    res = model.predict(data_pad)
    print(res, np.argmax(res), sep='\n')
except Exception as e:
    print(f"Произошла ошибка: {e}")

11..........................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import re

from tensorflow.keras.layers import Dense, GRU, Input, Dropout, Embedding
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from google.colab import files

try:
    # Загрузка файлов
    uploaded = files.upload()
    texts_true_file = next(iter(uploaded))  # Get the first uploaded file
    texts_false_file = next(iter(uploaded))  # Get the second uploaded file

    # Чтение данных
    with open(texts_true_file, 'r', encoding='utf-8') as f:
        texts_true = f.readlines()
        texts_true[0] = texts_true[0].replace('\ufeff', '')

    with open(texts_false_file, 'r', encoding='utf-8') as f:
        texts_false = f.readlines()
        texts_false[0] = texts_false[0].replace('\ufeff', '')

    texts = texts_true + texts_false
    count_true = len(texts_true)
    count_false = len(texts_false)
    total_lines = count_true + count_false
    print(count_true, count_false, total_lines)

    maxWordsCount = 1000
    tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–"—#$%&amp;()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r«»', lower=True,
                          split=' ', char_level=False, oov_token='<UNK>')  # Add oov_token
    tokenizer.fit_on_texts(texts)

    dist = list(tokenizer.word_counts.items())
    print(dist[:10])
    print(texts[0][:100])

    max_text_len = 20 #Увеличил длину

    data = tokenizer.texts_to_sequences(texts)
    data_pad = pad_sequences(data, maxlen=max_text_len)
    print(data_pad)

    print(list(tokenizer.word_index.items()))

    X = data_pad
    Y = np.array([[1, 0]] * count_true + [[0, 1]] * count_false)
    print(X.shape, Y.shape)

    indeces = np.random.choice(X.shape[0], size=X.shape[0], replace=False)
    X = X[indeces]
    Y = Y[indeces]

    model = Sequential()
    model.add(Embedding(maxWordsCount, 128, input_length=max_text_len))
    model.add(GRU(128, return_sequences=True))
    model.add(Dropout(0.5)) #Добавляем Dropout
    model.add(GRU(64))
    model.add(Dropout(0.5)) #Добавляем Dropout
    model.add(Dense(2, activation='softmax'))
    model.summary()

    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.0001))

    history = model.fit(X, Y, batch_size=32, epochs=50)

    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))

    def sequence_to_text(list_of_indices):
        words = [reverse_word_map.get(letter) or '<UNK>' for letter in list_of_indices] #Обрабатываем UNK
        return ' '.join(words)

    t = "Это шедевр изобразительного искусства".lower()
    data = tokenizer.texts_to_sequences([t])
    data_pad = pad_sequences(data, maxlen=max_text_len)
    print(sequence_to_text(data_pad[0])) #Используем data_pad[0]

    res = model.predict(data_pad)
    print(res, np.argmax(res), sep='\n')

except Exception as e:
    print(f"Произошла ошибка: {e}")

